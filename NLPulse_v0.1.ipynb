{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8PGfjPpVzlgc"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import requests\n",
        "import feedparser\n",
        "import sqlite3\n",
        "from datetime import datetime\n",
        "from transformers import pipeline\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('vader_lexicon', quiet=True)\n",
        "\n",
        "# Initialize NLTK's VADER sentiment analyzer\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Load pre-trained FinBERT model and tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"ProsusAI/finbert\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"ProsusAI/finbert\")\n",
        "\n",
        "# Load the additional transformers models\n",
        "roberta_tokenizer = AutoTokenizer.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment\")\n",
        "roberta_model = AutoModelForSequenceClassification.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment\")\n",
        "\n",
        "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "\n",
        "# Initialize RoBERTa sentiment analysis\n",
        "def roberta_sentiment_analysis(text):\n",
        "    inputs = roberta_tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
        "    outputs = roberta_model(**inputs)\n",
        "    roberta_probs = torch.nn.functional.softmax(outputs.logits, dim=1)\n",
        "    roberta_sentiment = torch.argmax(roberta_probs).item()\n",
        "\n",
        "    # Map RoBERTa sentiment (0 -> negative, 1 -> neutral, 2 -> positive) to a 0-10 scale\n",
        "    return roberta_sentiment * 5, torch.max(roberta_probs).item()\n",
        "\n",
        "# BART-based summarization for long texts\n",
        "def generate_summary(text):\n",
        "    if len(text) > 1000:\n",
        "        summary = summarizer(text, max_length=100, min_length=30, do_sample=False)\n",
        "        return summary[0]['summary_text']\n",
        "    return text\n",
        "\n",
        "def analyze_sentiment(text):\n",
        "    # Handle missing or short summaries\n",
        "    if not text or len(text.strip()) < 10:\n",
        "        return 5.0  # Neutral score for missing or insufficient text\n",
        "\n",
        "    # Generate summary if text is too long\n",
        "    summarized_text = generate_summary(text)\n",
        "\n",
        "    # VADER sentiment analysis\n",
        "    vader_score = sia.polarity_scores(summarized_text)\n",
        "    normalized_vader_score = (vader_score['compound'] + 1) * 5\n",
        "\n",
        "    # FinBERT sentiment analysis\n",
        "    inputs = tokenizer(summarized_text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
        "    outputs = model(**inputs)\n",
        "    finbert_probs = torch.nn.functional.softmax(outputs.logits, dim=1)\n",
        "    finbert_sentiment = torch.argmax(finbert_probs).item()\n",
        "    normalized_finbert_score = finbert_sentiment * 5\n",
        "\n",
        "    # RoBERTa sentiment analysis\n",
        "    roberta_score, roberta_confidence = roberta_sentiment_analysis(summarized_text)\n",
        "\n",
        "    # Confidence-based adjustment\n",
        "    finbert_confidence = torch.max(finbert_probs).item()  # Confidence of the highest FinBERT class\n",
        "    vader_confidence = abs(vader_score['compound'])  # VADER confidence derived from compound score\n",
        "    total_confidence = finbert_confidence + vader_confidence + roberta_confidence\n",
        "\n",
        "    # Assign weights based on confidence\n",
        "    finbert_weight = finbert_confidence / total_confidence\n",
        "    vader_weight = vader_confidence / total_confidence\n",
        "    roberta_weight = roberta_confidence / total_confidence\n",
        "\n",
        "    # Combine the scores based on weighted confidence\n",
        "    combined_score = (\n",
        "        (vader_weight * normalized_vader_score) +\n",
        "        (finbert_weight * normalized_finbert_score) +\n",
        "        (roberta_weight * roberta_score)\n",
        "    )\n",
        "\n",
        "    # Ensure the combined score remains between 0 and 10\n",
        "    combined_score = max(0, min(combined_score, 10))\n",
        "\n",
        "    return combined_score\n",
        "\n",
        "\n",
        "def setup_database():\n",
        "    conn = sqlite3.connect('news_sentiment.db')\n",
        "    c = conn.cursor()\n",
        "    c.execute('''CREATE TABLE IF NOT EXISTS sentiment_scores\n",
        "                 (date TEXT, time TEXT, title TEXT, summary TEXT, score REAL)''')\n",
        "    conn.commit()\n",
        "    return conn\n",
        "\n",
        "def store_score(conn, date, time, title, summary, score):\n",
        "    c = conn.cursor()\n",
        "    c.execute(\"INSERT INTO sentiment_scores VALUES (?, ?, ?, ?, ?)\",\n",
        "              (date, time, title, summary, score))\n",
        "    conn.commit()\n",
        "\n",
        "def fetch_news_and_analyze(url, conn):\n",
        "    response = requests.get(url)\n",
        "    feed = feedparser.parse(response.content)\n",
        "\n",
        "    for entry in feed.entries:\n",
        "        title = entry.title\n",
        "        summary = entry.summary\n",
        "        published = entry.published\n",
        "\n",
        "        # Parse the published date and time\n",
        "        dt = datetime.strptime(published, \"%a, %d %b %Y %H:%M:%S %z\")\n",
        "        date = dt.strftime(\"%Y-%m-%d\")\n",
        "        time = dt.strftime(\"%H:%M:%S\")\n",
        "\n",
        "        score = analyze_sentiment(summary)\n",
        "\n",
        "        print(f\"Title: {title}\")\n",
        "        print(f\"Published: {published}\")\n",
        "        print(f\"Summary: {summary}\")\n",
        "        print(f\"Sentiment Score: {score:.2f}\")\n",
        "        print(\"---\")\n",
        "\n",
        "        # Store in database\n",
        "        store_score(conn, date, time, title, summary, score)\n",
        "\n",
        "def get_daily_average(conn, date):\n",
        "    c = conn.cursor()\n",
        "    c.execute(\"SELECT AVG(score) FROM sentiment_scores WHERE date = ?\", (date,))\n",
        "    return c.fetchone()[0]\n",
        "\n",
        "# Usage\n",
        "url = \"https://timesofindia.indiatimes.com/rssfeedstopstories.cms\"\n",
        "\n",
        "# Setup database\n",
        "conn = setup_database()\n",
        "\n",
        "# Fetch news, analyze sentiment, and store in database\n",
        "fetch_news_and_analyze(url, conn)\n",
        "\n",
        "# Get today's date\n",
        "today = datetime.now().strftime(\"%Y-%m-%d\")\n",
        "\n",
        "# Calculate and print daily average\n",
        "daily_avg = get_daily_average(conn, today)\n",
        "print(f\"Average sentiment score for today ({today}): {daily_avg:.2f}\")\n",
        "\n",
        "# Close the database connection\n",
        "conn.close()\n",
        "\n",
        "print(\"Data has been stored in the 'news_sentiment.db' database.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install feedparser TextBlob\n",
        "!pip install nltk transformers torch"
      ],
      "metadata": {
        "id": "XelaHxAvzq3Q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
